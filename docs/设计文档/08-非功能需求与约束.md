# 非功能需求与约束

## 1. 性能需求 (Performance)

### 1.1 响应时间
- **交互响应**: 界面点击响应 < 100ms。
- **RAG 检索**: 向量检索 Top K < 500ms。
- **Agent 生成**: 流式输出 (Streaming)，首字响应时间 (TTFT) < 2s (依赖模型)。

### 1.2 并发与吞吐
- **扫描**: 支持后台异步扫描，大文件 (1MB+) 解析不阻塞 UI。
- **向量化**: 批量处理 Embedding 请求，避免触发 API Rate Limit。

## 2. 可靠性与鲁棒性
- **断点续传**: 扫描过程中断后，下次启动能从断点（或未处理文件）继续。
- **异常隔离**: 单个文件的解析错误不应导致整个书籍导入失败，应记录错误日志并跳过。
- **降级策略**: 当 LLM 服务不可用时，系统应退化为普通的 Markdown 阅读器，保留基本的浏览和搜索功能。

## 3. 可扩展性
- **LLM 适配**: 抽象 `ILLMService` 接口，支持 OpenAI, Azure OpenAI, Anthropic 及本地 Ollama 模型。
- **存储适配**: 数据库层设计为接口，理论上可替换 SQLite 为 PostgreSQL等。

## 4. 安全与隐私
- **Data Privacy**: 所有书籍内容、学习记录、向量数据默认存储在用户本地设备，不上传云端（除非用户配置使用云端 LLM，此时需脱敏或提示）。
- **API Key Security**: API Key 使用系统级安全存储 (如 Windows Credential Manager) 或加密存储，不在日志中明文打印。

## 5. 国际化 (i18n)
- 界面文案支持多语言。
- Prompt 模板支持多语言，根据书籍语言自动选择合适的 Prompt。

## 6. 环境依赖
- **Runtime**: .NET 8+ / Node.js (Frontend)。
- **OS**: Windows / macOS / Linux。
- **Hardware**: 推荐 8GB+ RAM (若运行本地 LLM/Embedding 需更高配置)。
```