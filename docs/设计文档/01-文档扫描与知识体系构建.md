# 文档扫描与知识体系构建

## 1. 功能概述

负责将原始 Markdown 文档转化为计算机可理解、可检索的知识结构。
核心产出包括：

1. **结构化知识库**：章节树、知识点及其关联。
2. **向量索引 (RAG)**：用于语义检索的文本向量库。
3. **原文索引**：用于精确回溯的文件位置映射。

## 2. 输入与配置

- **Config**：包含多个 `BookRoot` 配置（路径、排除规则、参考目录名）。
- **State**：上次扫描的文件 Hash 记录（用于增量检测）。

## 3. 扫描流程

### 3.1 文件发现与预处理

1. **遍历**：递归扫描 `root_path`。
2. **分类**：
    - **主文档**：参与知识体系构建的章节文档。
    - **参考资料**：位于 `reference_dirs` 下的文档，仅建立 RAG 索引，不生成知识点节点。
3. **增量检测**：计算文件 `MD5/SHA256`。
    - **新增/修改**：标记为待处理。
    - **删除**：标记为待清理。
    - **未变**：跳过。

### 3.2 文本切分 (Chunking)

为了兼顾 RAG 检索效果与原文展示：

- **按语义切分**：优先按 Markdown 标题 (`#`), 段落换行进行切分。
- **滑动窗口**：对于超长段落，采用滑动窗口切分，保留重叠上下文。
- **元数据保留**：每个 Chunk 需保留 `SourceFilePath`, `StartLine`, `EndLine`, `HeadingPath`。

### 3.3 向量化 (Embedding)

- 对所有变更的 Chunk 调用 Embedding 模型（如 `text-embedding-ada-002` 或本地 BERT）。
- 存入向量数据库。
- **引用资料处理**：参考资料目录下的文档也进行同样的 Chunking 和 Embedding，但在 Metadata 中标记 `Type=Reference`。

### 3.4 知识点提取 (Knowledge Extraction Agent)

仅针对 **主文档** 进行处理：

1. **结构提取**：
    - 解析 Markdown AST，直接构建章节目录树。
2. **概念提取 (Concept Extraction)**：
    - 输入：章节文本内容。
    - Prompt：识别核心概念、定义、关键步骤。
    - 输出：JSON 格式的 KP 列表，包含 `Title`, `Type`, `Summary`。
3. **关系构建**：
    - **层级关系**：KP 自动挂载到所属章节下。
    - **语义关联**：Agent 识别 KP 之间的关系（如 A 是 B 的前置，A 对比 B）。
    - 并在后续版本中支持全库扫描进行跨章节链接。

## 4. 数据更新策略 (增量与一致性)

- **新增文件**：执行完整流程，插入数据。
- **修改文件**：
  1. 删除旧的 Chunks 向量。
  2. 删除旧的 KPs（如果 KP ID 发生变化，或者关联关系失效需重新评估）。
  3. 执行完整流程。
- **删除文件**：级联删除对应的 Chunks、Vectors、KPs。

## 5. 质量控制

- **幻觉检测**：提取的 KP 必须能回溯到具体的 Chunk。
- **去重**：同一章节下的同义词 KP 进行合并。
- **垃圾过滤**：过滤掉过短的、无实质内容的标题（如 "小结", "前言"）。

## 6. 输出数据结构 (DTO)

详见数据模型文档，核心包括：

- `KnowledgeGraph`: 节点与边。
- `DocumentIndex`: 文件列表与 Hash。
- `VectorIndex`: 向量库引用。

```

